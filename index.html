<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination </title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a href="https://automation.seu.edu.cn/wt/list.htm">Teng Wang</a><sup>*,1</sup>,</span>
              <span class="author-block"><a href="https://github.com/200203Z">Xinxin Zhao</a><sup>*,1</sup>,</span>
              <span class="author-block"><a href="https://wzcai99.github.io/">Wenzhe Cai</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://automation.seu.edu.cn/szy/list.htm">Changyin Sun</a><sup>1</sup>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Southeast University</span>
              <span class="author-block"><sup>2</sup>Shanghai AI Laboratory</span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      
                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2505.08712" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                        </a>
                      </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling the execution of long-horizon tasks such as object search. 
            While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning processes remain constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry-critical factors for informed navigation decisions. 
            In this work, we explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this by developing the imagination-powered navigation framework <b>ImagineNav++</b>, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLMs. 
            Specifically, we first introduce a <b>future-view imagination</b> module, which distills human navigation preferences to generate semantically meaningful candidate viewpoints with high exploration potential. 
            These imagined future views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a <b>selective foveation memory</b> mechanism, which hierarchically integrate keyframe observations through a sparse-to-dense framework, thereby constructing a compact yet comprehensive memory for long-term spatial reasoning. 
            This integrated approach effectively transforms the challenging goal-oriented navigation problem into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks demonstrate that our ImagineNav++ achieves SOTA performance in mapless setting, even surpassing most cumbersome map-based methods, revealing the importance of scene imagination and scene memory in VLM-based spatial reasoning.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- End image carousel -->

<!-- Framework. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-11">
        <h2 class="title is-3">Approach</h2>
        <img src="./static/images/approach.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-11">
        <div class="content has-text-justified">
          <p>The framework consists of four key components: the <b>future-view imagination module (Where2Imagine + NVS)</b>, the <b>selective foveation memory module</b>, the <b>VLM-based high-level planner</b>, and the <b>low-level PointNav controller</b>. 
            Through an iterative cycle of imagination, reasoning, and execution, it decomposes long-horizon goal-oriented navigation into tractable sub-tasks without requiring explicit mapping.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

 
<!-- results -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-11">
        <h2 class="title is-3">Results</h2>
      </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-11">
        <!-- 第一个实验：图片和文字 -->
        <div class="has-text-centered mb-6">
          <img src="./static/images/exp1.png"
               class="interpolation-image mb-4"
               alt="Experiment 1 results"/>
          <div class="content has-text-justified">
            <p><b>Table I.</b> Comparison with previous work on <b>object-goal</b> navigation.</p>
          </div>
        </div>
        
        <!-- 第二个实验：图片和文字 -->
        <div class="has-text-centered">
          <img src="./static/images/exp2.png"
               class="interpolation-image mb-4"
               alt="Experiment 2: Real-to-Sim Reconstruction"/>
          <div class="content has-text-justified">
            <p><b>Table II.</b> Comparison with previous work on <b>instance-image-goal</b> navigation.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Results -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- 主标题 -->
    <h1 class="title is-2 has-text-centered has-text-weight-bold mb-6">Navigation Demo</h1>
    <div class="hero-body">
      <!-- 第一个视频 -->
      <div class="video-container" style="margin-bottom: 3rem;">
        <video poster="" id="video1" autoplay controls muted loop height="100%">
          <source src="./static/videos/plant.mp4" type="video/mp4">
        </video>
      </div>
      
      <!-- 第二个视频 -->
      <div class="video-container" style="margin-bottom: 3rem;">
        <video poster="" id="video2" autoplay controls muted loop height="100%">
          <source src="./static/videos/bed.mp4" type="video/mp4">
        </video>
      </div>
      
      <!-- 第三个视频 -->
      <div class="video-container">
        <video poster="" id="video3" autoplay controls muted loop height="100%">
          <source src="./static/videos/couch.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>
  
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-centered has-text-centered">
      <h2 class="title is-3">Navigation Demo</h2> 
      <div id="results-carousel" class="carousel results-carousel is-centered has-text-centered is-max-desktop content">
        
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop width="75%">
            <source src="./static/videos/plant.mp4"
            type="video/mp4">
          </video>
        </div>
        
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop width="75%">
            <source src="./static/videos/bed.mp4"
            type="video/mp4">
          </video>
        </div>
        
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop width="75%">
            <source src="./static/videos/couch.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{cai2025navdp,
  title = {NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance},
  author = {Wenzhe Cai, Jiaqi Peng, Yuqiang Yang, Yujian Zhang, Meng Wei, Hanqing Wang, Yilun Chen, Tai Wang and Jiangmiao Pang},
  booktitle = {Arxiv},
  year = {2025},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
